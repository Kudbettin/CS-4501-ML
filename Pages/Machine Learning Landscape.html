<link rel="stylesheet" href="markdown.css" />
<p><a href="../index.html">Home</a></p>
<h1 id="machine-learning-landscape">Machine Learning Landscape</h1>
<ul>
<li>Instead of writing rules (traditional approach), you train a ML algorithm with data and it determines its own rules.
<ul>
<li>This can help you gain insights because you can inspect the solution generated by the ML algorithm which can help you find patterns and learn more about the problem.
<ul>
<li>This is called <strong>data mining</strong>.</li>
</ul></li>
</ul></li>
<li>Machine learning is great for the following types of problems:
<ul>
<li>Problems which require lots of hand-tuning and long list of rules
<ul>
<li>Problems that have a bunch of if-statements</li>
</ul></li>
<li>Complex problems for which there is no good traditonal solution</li>
<li>Changing environment and need to adapt to new data
<ul>
<li>It becomes impractical to continuously update your rules as more data comes in</li>
</ul></li>
<li>Getting insights from large amounts of data</li>
</ul></li>
</ul>
<h2 id="types-of-algorithms">Types of algorithms</h2>
<h3 id="superverised">Superverised</h3>
<ul>
<li>Supervised learning is when you have a label - you predict a label based on a model generated by a training set
<ul>
<li>Each instance (inputs) in the training set is associated with a label (outcome)</li>
</ul></li>
<li>Examples:
<ul>
<li>k-Nearest Neighbors</li>
<li>Linear Regression
<ul>
<li>Predicting a numeric value</li>
</ul></li>
<li>Logistic Regression</li>
<li>Support Vector Machine</li>
<li>Decision Trees</li>
<li>Random Forests</li>
<li>Neural Networks</li>
</ul></li>
</ul>
<h3 id="unsupervised-learning-without-a-teacher">Unsupervised (learning without a teacher)</h3>
<ul>
<li>There is no need for labeling</li>
<li>Examples:
<ul>
<li>Clustering
<ul>
<li>Cluster data into groups based on characteristics</li>
</ul></li>
<li>Dimensionality reduction
<ul>
<li>Simplifies the data without losing too much information</li>
<li>merges correlated features into one
<ul>
<li>For example, car’s mileages may be very correlated with its age, so you can keep just one of the features</li>
</ul></li>
</ul></li>
<li>Anomoly detection
<ul>
<li>The algorithm can detect that an instance is an outlier</li>
</ul></li>
<li>K-means</li>
<li>Hierarchical clustering (HAC)</li>
<li>Principal component analysis (PCA)</li>
<li>Localy-linear embedding (LLE)</li>
</ul></li>
</ul>
<h3 id="reinforcement-learning">Reinforcement Learning</h3>
<ul>
<li>In some environment, you have some task. The agent starts naive and doesn’t know what to do. It recieves an award or a penalty based on whether or not it does the right thing. Iterate until an optimal policy is determined
<ul>
<li>Video game learning videos, such as the SethBlings or Codebullet videos</li>
</ul></li>
</ul>
<h3 id="batch-learning-offline-learning">Batch Learning (offline learning)</h3>
<ul>
<li>Must be trained using <strong>all available data</strong> at once.</li>
<li>Generally takes time, so typically done <strong>offline</strong></li>
<li>Requires a lot of computing resources (CPU, memory, network I/O, ect.)</li>
<li>Must be retrained from scratch for updated data</li>
</ul>
<h3 id="incremental-learning-online-learning">Incremental Learning (online learning)</h3>
<ul>
<li>New data is learned on the fly rather than all at once</li>
<li>Saves computing resources</li>
<li>Good for large data sets that can’t be contained in memory</li>
<li><strong>Learning rate</strong>: how quickly a ML algorithm should adapt to changing data</li>
</ul>
<h3 id="instance-based">Instance-Based</h3>
<ul>
<li>When you have different training instances plot, and new instances are compared to other instances similar to them in order to determine the label.
<ul>
<li>Classifying new instances based on is similarities to known instances</li>
</ul></li>
</ul>
<h3 id="model-based">Model-Based</h3>
<ul>
<li>You learn a curve that separates the data, and then you classify the new instanced based on where the new instance is relative to the curve</li>
</ul>
<h2 id="issues-that-can-happen-with-model-data">Issues that can happen with model data</h2>
<ul>
<li>Non enough training data</li>
<li>Non-representative training data
<ul>
<li>Sampling noise: error associated with sampling a small dataset</li>
<li>Sampling bias: A large portion of data is not representative due to sampling method</li>
</ul></li>
<li>Poor quality data - <strong>where Data Scientists spend most their time</strong>
<ul>
<li>Full of errors (human and machine)</li>
<li>missing data (nonresponse)</li>
<li>outliers</li>
<li>noise (by measurements)</li>
</ul></li>
<li>Irrelevant Features
<ul>
<li>GIGO principle: training data must have enough relevant features and not too many irrelevant ones (feature engineering)</li>
<li>Feature Selection: find useful ones</li>
<li>Feature extraction: combine existing features to make more useful ones</li>
<li>Feature creation: create new features by collecting new data</li>
</ul></li>
</ul>
<h2 id="issues-that-can-happen-with-model-fitting">Issues that can happen with model fitting</h2>
<ul>
<li>Overfitting the training data
<ul>
<li>The model performs well on training data, but does not generalize well</li>
<li>Overcome this with regularizations, which is when you make assumptions about the true model. For example, you might make a constraint that the slope can’t exceed some value so that the slope doesn’t increase as fast.
<ul>
<li>Uses a hyperparamenter, a parameter controlled by the learning algorithm rather than the model</li>
</ul></li>
</ul></li>
<li>Underfitting the training data
<ul>
<li>The model is too simple to learn the underlying model.</li>
</ul></li>
</ul>
<h2 id="testing-and-validating-your-algorithm">Testing and validating your algorithm</h2>
<ul>
<li>Split data into 2-3 partitions:
<ul>
<li>Training set: train your model
<ul>
<li>Has an associated training error</li>
</ul></li>
<li>Testing set: evaluate your model
<ul>
<li>Has an associated generalization error</li>
</ul></li>
<li>Validating set: if training error is low and generalization error is high, your algorithm overfits. Using validating set, choose value of a hypterparameter to avoid overfitting.</li>
</ul></li>
<li>Cross Validation to avoid wasting too much training data in validation sets
<ul>
<li>Split data into complementary subsets, each model against a different combination of these subsets, and validate against the remaining parts.</li>
<li>Select the model type and hyperparameters which yields small training errors</li>
<li>The final model is trained using the hyperparameters on full training set Measure the generalized error on the test set.</li>
</ul></li>
</ul>
<p><a href="../index.html">Home</a></p>
