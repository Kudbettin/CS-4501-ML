# 1. Machine Learning Landscape

- Instead of writing rules (traditional approach), you train a ML algorithm with data and it determines its own rules.
  - This can help you gain insights because you can inspect the solution generated by the ML algorithm which can help you find patterns and learn more about the problem.
    - This is called **data mining**.
- Machine learning is great for the following types of problems:
  - Problems which require lots of hand-tuning and a long list of rules
    - Problems that have a bunch of if-statements
  - Complex problems for which there is no good traditional solution
  - Changing environment and need to adapt to new data
    - It becomes impractical to continuously update your rules as more data comes in
  - Getting insights from large amounts of data

## Types of algorithms

### Supervised
- Supervised learning is when you have a label - you predict a label based on a model generated by a training set
  - Each instance (inputs) in the training set is associated with a label (outcome)
- Examples:
  - k-Nearest Neighbors
  - Linear Regression
    - Predicting a numeric value
  - Logistic Regression
  - Support Vector Machine
  - Decision Trees
  - Random Forests
  - Neural Networks

### Unsupervised (learning without a teacher)
- There is no need for labeling
- Examples:
  - Clustering
    - Cluster data into groups based on characteristics
  - Dimensionality reduction
    - Simplifies the data without losing too much information
    - merges correlated features into one
      - For example, a car's mileages may be very correlated with its age, so you can keep just one of the features
  - Anomaly detection
    - The algorithm can detect that an instance is an outlier
  - K-means
  - Hierarchical clustering (HAC)
  - Principal component analysis (PCA)
  - Locally-linear embedding (LLE)

### Reinforcement Learning
- In some environment, you have some task.  The agent starts naive and doesn't know what to do.  It receives an award or a penalty based on whether or not it does the right thing.  Iterate until an optimal policy is determined.

### Batch Learning (offline learning)
- Must be trained using **all available data** at once.
- Generally takes time, so typically done **offline**
- Requires a lot of computing resources (CPU, memory, network I/O, etc.)
- Must be retrained from scratch for updated data

### Incremental Learning (online learning)
- New data is learned on the fly rather than all at once
- Saves computing resources
- Good for large datasets that can't be contained in memory
- **Learning rate**: how quickly a ML algorithm should adapt to changing data

### Instance-Based
- When you have different training instances plot, and new instances are compared to other instances similar to them in order to determine the label.
  - Classifying new instances based on is similarities to known instances

### Model-Based
- You learn a curve that separates the data, and then you classify the new instanced based on where the new instance is relative to the curve

## Issues that can happen with model data
- Non enough training data
- Non-representative training data
  - Sampling noise: error associated with sampling a small dataset
  - Sampling bias: A large portion of data is not representative due to the sampling method
- Poor quality data - **where Data Scientists spend most their time**
  - Full of errors (human and machine)
  - missing data (nonresponse)
  - outliers
  - noise (by measurements)
- Irrelevant Features
  - GIGO principle: training data must have enough relevant features and not too many irrelevant ones (feature engineering)
  - Feature Selection: find useful ones
  - Feature extraction: combine existing features to make more useful ones
  - Feature creation: create new features by collecting new data

## Issues that can happen with model fitting
- Overfitting the training data
  - The model performs well on training data but does not generalize well
  - Overcome this with regularizations, which is when you make assumptions about the true model.  For example, you might make a constraint that the slope can't exceed some value so that the slope doesn't increase as fast.
    - Uses a hyperparameter, a parameter controlled by the learning algorithm rather than the model
- Underfitting the training data
  - The model is too simple to learn the underlying model.

## Testing and validating your algorithm
- Split data into 2-3 partitions:
  - Training set: train your model
    - Has an associated training error
  - Testing set: evaluate your model
    - Has an associated generalization error
  - Validating set: if training error is low and generalization error is high, your algorithm overfits.  Using the validating set, choose the value of a hyperparameter to avoid overfitting.
- Cross Validation to avoid wasting too much training data in validation sets
  - Split data into complementary subsets, each model against a different combination of these subsets, and validate against the remaining parts.
  - Select the model type and hyperparameters which yields small training errors
  - The final model is trained using the hyperparameters on full training set
Measure the generalized error on the test set.


